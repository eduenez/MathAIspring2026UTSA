{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST digits recognition\n",
        "## Feedforward network with 2 hidden layers\n",
        "Uses [Keras](https://keras.io/) with [JAX](https://github.com/jax-ml/jax) as NN engine."
      ],
      "metadata": {
        "id": "sc5TbaLmrdBD"
      },
      "id": "sc5TbaLmrdBD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84fdcb2e",
      "metadata": {
        "id": "84fdcb2e"
      },
      "outputs": [],
      "source": [
        "# Install Jax running on Google/Colab Tensor Processing Units\n",
        "!pip install \"jax[tpu]\"\n",
        "\n",
        "# Install Keras\n",
        "!pip install keras-cv\n",
        "!pip install keras-hub\n",
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7adfac48",
      "metadata": {
        "id": "7adfac48"
      },
      "outputs": [],
      "source": [
        "# Import installed software and put things in place\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "import keras\n",
        "# print(keras.__version__)\n",
        "# print(keras.backend.backend())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model `model` below is a feedforward network\n",
        "\n",
        "- Input layer of dimension `784` (each MNIST character is encoded as a 28-by-28 image having `28*28 = 784` pixels when “flattened”).\n",
        "- Dense (“fully connected”) hidden layer 1 of dimension `64`.\n",
        "- Dense (“fully connected”) hidden layer 2 of dimension `64`.\n",
        "- Dense (“fully connected”) output layer of dimension `10`.\n",
        "\n",
        "The `model` is then created as a Keras model object.\n",
        "It will process `784`-dimensional inputs and generate `10`-dimensional outputs.\n",
        "\n",
        "- Each of the 64 dimensions of the 1st layer has 784 linear cofficients plus 1 bias = 785 parameters, for a total of 64*785 = `50,240` parameters.\n",
        "- Each of the 64 dimensions of the 1st layer has 64 linear cofficients plus 1 bias = 65 parameters, for a total of 64*65 = `4,160` parameters.\n",
        "- Each of the 10 dimensions of the 1st layer has 64 linear cofficients plus 1 bias = 65 parameters, for a total of 10*65 = `650` parameters.\n",
        "\n",
        "Therefore, `model` has `50,240 + 4,160 + 650 = 55,050` total parameters.\n",
        "\n",
        "**Note:** The MNIST data set has *not* yet been loaded at all!\n",
        "Absolutely *no* data proper has been used at this point.\n"
      ],
      "metadata": {
        "id": "YYq7rJkwsqJP"
      },
      "id": "YYq7rJkwsqJP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60bd502f",
      "metadata": {
        "id": "60bd502f"
      },
      "outputs": [],
      "source": [
        "dim_hl_1 = 64 # @param {type: \"integer\"}\n",
        "dim_hl_2 = 64 # @param {type: \"integer\"}\n",
        "\n",
        "inputs = keras.Input(shape=(784,))  # Input layer shape for flatened MNIST images\n",
        "hidden1 = layers.Dense(dim_hl_1, activation=\"relu\")(inputs)  # First hidden layer\n",
        "hidden2 = layers.Dense(dim_hl_2, activation=\"relu\")(hidden1)  # Second hidden layer\n",
        "# Output layer must have dimension 10 = number of categories.\n",
        "outputs = layers.Dense(10)(hidden2)  # No ReLu here b/c output is \"logits\".\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac8ee8f7",
      "metadata": {
        "id": "ac8ee8f7"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, \"feedforward_model.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6575025a",
      "metadata": {
        "id": "6575025a"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, \"feedforward_model_with_shape_info.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load MNIST dataset\n",
        "\n",
        "  Each single MNIST image of a handwritten digit 0, 1, 2, …, 8, 9 is encoded as a matrix of size `28*28` whose entries are 8-bit unsigned integers (i.e., each entry is an integer in the range from `0` to `2^8 - 1 = 255`).\n",
        "Each integer represents the grayscale color of a pixel (0 = black, 255 = white).\n",
        "\n",
        "For reasons of computational efficiency and numerical stability, the grayscale is rescaled to be a floating-point (decimal) number in the interval `[0, 1)`, and the 28-by-28 matrix is “flattened” to be a single (column) vector of `28*28 = 784` decimal such entries, each a `float32` (32-bit floating point number).\n",
        "\n",
        "Since the training set consists of 60,000 such digit images, each effectively a column vector of size 784, it gives a large matrix `x_train` of size `60,000 × 784`; similarly, the test set of 10,000 images gives a `10,000 × 784` matrix `x_test`.\n"
      ],
      "metadata": {
        "id": "ebetN81k2JeN"
      },
      "id": "ebetN81k2JeN"
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
        "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255"
      ],
      "metadata": {
        "id": "7FTwVzFM2sLf"
      },
      "id": "7FTwVzFM2sLf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile and train the model\n",
        "\n",
        "By default, when a model is “compiled”, its parameters are initialized randomly and uniformly (with zero mean and variance determined by the input and output dimensions at each layer—but otherwise not of immediate interest to us right now).\n",
        "\n",
        "When `model` is applied to an input of size `m × 764`, it is applied to *each* of the `m` columns, one at a time—because `model` only accepts as input a (column) vector of size 764, and outputs one of size 10;\n",
        "in other words, `model` sees those `m` *rows* as completely independent of each other.\n",
        "The result is a matrix of size `m × 10`.\n",
        "\n",
        "`batch_size=64` means that each “training step” of the model uses *only* a randomly-chosen batch of (about) `m = 64` inputs (i.e., 64 rows of `x_train`) at a time, until all 60,000 rows are exhausted.\n",
        "This called a (single) “epoch”, which consists of (about) `60,000/64 ≈ 934` such steps.\n",
        "The process is restarted (bringing back all 60,000 rows of `x_train`) and carried out a total number of epochs `num_epochs = 20`.\n",
        "\n",
        "(Actually, `validation_split=0.2` means that 20% of the training data is excluded during each epoch, so an epoch therefore only consists of about `0.8 * 934 ≈ 747` batches of `≈ 64` training images each.)\n",
        "\n",
        "Keras has various built-in optimization algorithms; this model's algorithm is the *root mean-square propagation* or `RMSprop`.\n",
        "\n",
        "The accuracy of the model is quantified using a \"loss\" function. A larger loss means less accuracy, so the goal as the model is trained is *reducing* or “minimizing” the loss.\n",
        "(The loss function used in this implementation is called *categorical cross-entropy*. Entropy and related quantities will be introduced later in the semester.)"
      ],
      "metadata": {
        "id": "Dr22LcmX2uH1"
      },
      "id": "Dr22LcmX2uH1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e24e0d9",
      "metadata": {
        "id": "7e24e0d9"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20  # @param {type: \"integer\"}\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=64, epochs=20, validation_split=0.2)\n",
        "\n",
        "test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(\"Test loss:\", test_scores[0])\n",
        "print(\"Test accuracy:\", test_scores[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1109a302",
      "metadata": {
        "id": "1109a302"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "454aa1a6",
      "metadata": {
        "id": "454aa1a6"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below randomly chooses 5 of the 10,000 images in the test set and applies `model` to assign a label (0, 1, …, 9) using a straightforward likelihood maximization procedure.\n",
        "Very few images in the test set should be misclassified:\n",
        "the `True` and `Pred`icted labels should be in agreement."
      ],
      "metadata": {
        "id": "jE2CH7WPA4z-"
      },
      "id": "jE2CH7WPA4z-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "738e5523",
      "metadata": {
        "id": "738e5523"
      },
      "outputs": [],
      "source": [
        "# Select a few random indices from the test set\n",
        "num_samples = 5\n",
        "indices = np.random.choice(len(x_test), num_samples, replace=False)\n",
        "\n",
        "plt.figure(figsize=(10, 2))\n",
        "for i, idx in enumerate(indices):\n",
        "    img = x_test[idx]\n",
        "    # If images are flattened, reshape to 28x28\n",
        "    if img.shape[-1] != 28:\n",
        "        img = img.reshape(28, 28)\n",
        "    plt.subplot(1, num_samples, i + 1)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    # Predict label and compare with true label.\n",
        "    pred = model.predict(np.expand_dims(x_test[idx], axis=0), verbose=0)\n",
        "    pred_label = np.argmax(pred, axis=1)[0]\n",
        "    true_label = y_test[idx]\n",
        "    plt.title(f\"True: {true_label}\\nPred: {pred_label}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}